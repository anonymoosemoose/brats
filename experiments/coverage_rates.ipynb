{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage Rate of Built-In Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes a simulation study to evaluate the coverage rates of our proposed built-in intervals. Each intervals are designed to capture:\n",
    "1. Confidence Intervals: Ground truth $f(x)$\n",
    "2. Prediction Intervals: Next observation $f(x)+\\varepsilon$.\n",
    "3. Reproduction Intervals: Next prediction $\\widehat{f}(x)$\n",
    "4. Conformal Prediction Intervals: Next observation\n",
    "\n",
    "To evaluate the performance of proposed intervals, we train `reps` number of models and we sample a set of test points. We offer two ways of calculating the average coverage rates of an interval:\n",
    "\n",
    "1. We first compute the coverage at each test point by averaging the boolean `covered` over models. Then we average the coverage rates to get an average coverage rate.\n",
    "2. We first compute the coverage rate using the average of the boolean `covered` of a single model over all test points. Then we take average of that and output the average coverage rate.\n",
    "\n",
    "Empirically the second coverage rates are much better than the first one. The rationale is as follow: Some points are just hard to be captured by our models. So averaging over models first will increase the weight of those 'bad' points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"../src\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "# Prevent ptitprince from raising cmap errors\n",
    "cm.register_cmap = lambda *args, **kwargs: None\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from BRAT.algorithms import BRATD\n",
    "from BRAT.utils import generate_data, find_min_scale\n",
    "import ptitprince as pt\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "plt.style.use('../matplotlibrc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelizing the simulation\n",
    "mp.set_start_method('fork', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrating the intervals\n",
    "def calibrate_scale(widths_cal, y_cal, y_pred_cal, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Two-stage search for global c so that empirical coverage â‰ˆ 1-alpha.\n",
    "    \"\"\"\n",
    "    n = len(y_cal)\n",
    "    def cov(c):\n",
    "        lo = y_pred_cal - c * widths_cal\n",
    "        hi = y_pred_cal + c * widths_cal\n",
    "        return np.mean((y_cal >= lo) & (y_cal <= hi))\n",
    "\n",
    "    # exponential search\n",
    "    c = 1.0\n",
    "    if cov(c) < 1 - alpha:\n",
    "        while cov(c) < 1 - alpha:\n",
    "            c *= 2\n",
    "    C = c\n",
    "\n",
    "    # binary refine on [0, C]\n",
    "    lo, hi = 0.0, C\n",
    "    tol = 2.0 / n\n",
    "    while hi - lo > tol:\n",
    "        mid = (lo + hi) / 2\n",
    "        if cov(mid) < 1 - alpha:\n",
    "            lo = mid\n",
    "        else:\n",
    "            hi = mid\n",
    "    return hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below run the simulation on a Friedman function:\n",
    "\n",
    "$$\n",
    "y=10\\sin(\\pi x_1\\cdot x_2) + 20(x_3-0.5)^2 + 10x_4 + 5x_5 + \\varepsilon\n",
    "$$\n",
    "\n",
    "All related data are stored to `./coverage/data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During previous experiments, we explore around some combinations of hps to find patterns. Hence the grid search.\n",
    "# Some patterns we found: \n",
    "# 1. What leads to a wide interval: Low learning rate, low dropout rate, low subsampling rate.\n",
    "# 2. What leads to a narrow interval: Undersmoothing (the contrary)\n",
    "\n",
    "# Hyperparameter grid. Add parameters if you'd like to explore around them!\n",
    "lr_values = [0.6]\n",
    "dr_values = [0.3]\n",
    "sr_values = [0.8]\n",
    "max_depth_values = [4]\n",
    "\n",
    "# Data generation configurations\n",
    "num_test_pts = 200\n",
    "rep = 30\n",
    "epoch = 200\n",
    "Nystrom_subsample = 0.1\n",
    "in_bag = False\n",
    "function = 'friedman1'\n",
    "\n",
    "# Create directories for saving results\n",
    "os.makedirs(\"../experiments/coverage/data\", exist_ok=True)\n",
    "os.makedirs(\"../experiments/coverage/plots\", exist_ok=True)\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for idx, (lr, dr, sr, max_depth) in enumerate(product(lr_values, dr_values, sr_values, max_depth_values)):\n",
    "    # Data generation\n",
    "    X_train, y_train, X_test, y_test, y_test_true, X_cal, y_cal = generate_data(\n",
    "        function_type=function, \n",
    "        n_train=1000, \n",
    "        n_test=200,\n",
    "        n_calibration=200, \n",
    "        noise_std=0.1,\n",
    "        seed=idx\n",
    "    )\n",
    "    test_points = X_test[:num_test_pts]\n",
    "    y_true = y_test_true[:num_test_pts]\n",
    "    y_obs = y_test[:num_test_pts]\n",
    "\n",
    "    # Training repetitions\n",
    "    def training_rep(k):\n",
    "        bratd = BRATD(\n",
    "            n_estimators=epoch,\n",
    "            learning_rate=lr,\n",
    "            subsample_rate=sr,\n",
    "            dropout_rate=dr,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=2,\n",
    "            disable_tqdm=True\n",
    "        )\n",
    "        \n",
    "        # Data generation\n",
    "        X_train, y_train, X_test, y_test, y_test_true, X_cal, y_cal = generate_data(\n",
    "            function_type=function, \n",
    "            n_train=1000, \n",
    "            n_test=200,\n",
    "            n_calibration=200, \n",
    "            noise_std=0.1, \n",
    "            seed=k\n",
    "        ) \n",
    "        test_points = X_test[:num_test_pts]\n",
    "        y_true = y_test_true[:num_test_pts]\n",
    "        y_obs = y_test[:num_test_pts]\n",
    "        \n",
    "        bratd.fit(X_train, y_train, X_test, y_test)\n",
    "        _, _, _ = bratd.unif_nystrom(Nystrom_subsample)\n",
    "        bratd.sketch_K()\n",
    "        sigma_hat2 = bratd.est_sigma_hat2(in_bag)\n",
    "        sigma_hat = np.sqrt(sigma_hat2)\n",
    "        lam = bratd.learning_rate\n",
    "        q = 1 - bratd.dropout_rate\n",
    "        s = (1 + lam * q) / lam\n",
    "\n",
    "        # calibration\n",
    "        y_pred_cal = []\n",
    "        tau_cal    = []\n",
    "        pi_sigma_cal = []\n",
    "        for x_cal in X_cal:\n",
    "            y_pc = bratd.predict(x_cal.reshape(1, -1)).item()\n",
    "            rn_c = bratd.sketch_r(x_cal.reshape(1, -1), vector=False).item()\n",
    "            t_c  = s * rn_c * sigma_hat\n",
    "            p_c  = np.sqrt(sigma_hat2 + t_c**2)\n",
    "\n",
    "            y_pred_cal.append(y_pc)\n",
    "            tau_cal.append(t_c)\n",
    "            pi_sigma_cal.append(p_c)\n",
    "\n",
    "        y_pred_cal    = np.array(y_pred_cal)    # shape (n_cal,)\n",
    "        widths_pi_cal = np.array(pi_sigma_cal)  # shape (n_cal,)\n",
    "\n",
    "        # 2) Calibrate once per interval type\n",
    "        c = calibrate_scale(widths_pi_cal, y_cal, y_pred_cal)\n",
    "\n",
    "        for i, x in enumerate(test_points):\n",
    "            y_pred = bratd.predict(x.reshape(1, -1))\n",
    "            rn_norm = bratd.sketch_r(x.reshape(1, -1))\n",
    "            tau_hat = s * rn_norm * sigma_hat\n",
    "            pi_sigma = np.sqrt(sigma_hat2 + tau_hat**2)\n",
    "\n",
    "            y_pred_cal = bratd.predict(X_cal)\n",
    "\n",
    "            ci_lower = y_pred - tau_hat\n",
    "            ci_upper = y_pred + tau_hat\n",
    "            ci_width = ci_upper - ci_lower\n",
    "            ci_covered = int(ci_lower.item() <= y_true[i] <= ci_upper.item())\n",
    "\n",
    "            pi_lower = y_pred - pi_sigma\n",
    "            pi_upper = y_pred + pi_sigma\n",
    "            pi_width = pi_upper - pi_lower\n",
    "            ri_lower = y_pred - np.sqrt(2) * tau_hat\n",
    "            ri_upper = y_pred + np.sqrt(2) * tau_hat\n",
    "            ri_width = ri_upper - ri_lower\n",
    "\n",
    "            y_pred_cal = bratd.predict(X_cal)\n",
    "            res_cal = np.abs(y_pred_cal - y_cal)\n",
    "            qtle_hat = np.quantile(res_cal, 1 - 0.05)\n",
    "            cfml_lower = y_pred - qtle_hat\n",
    "            cfml_upper = y_pred + qtle_hat\n",
    "            cfml_covered = int(cfml_lower.item() <= y_obs[i] <= cfml_upper.item())\n",
    "            pi_covered = int(pi_lower.item() <= y_obs[i] <= pi_upper.item())\n",
    "\n",
    "            ci_lower_scaled, ci_upper_scaled = y_pred- c * ci_width/2, y_pred + c * ci_width/2\n",
    "            pi_lower_scaled, pi_upper_scaled = y_pred- c * pi_width/2, y_pred + c * pi_width/2\n",
    "            ri_lower_scaled, ri_upper_scaled = y_pred- c * ri_width/2, y_pred + c * ri_width/2\n",
    "            \n",
    "            return {\n",
    "                \"model_idx\": k,\n",
    "                \"pt_idx\": i,\n",
    "                \"y_true\": y_true[i].item(),\n",
    "                \"y_obs\": y_obs[i].item(),\n",
    "                \"y_pred\": y_pred.item(),\n",
    "                \"bias\": y_pred.item() - y_true[i].item(),\n",
    "                \"calibration\": c,\n",
    "                \"ci_lower\": ci_lower.item(),\n",
    "                \"ci_upper\": ci_upper.item(),\n",
    "                \"ci_width\": ci_upper.item() - ci_lower.item(),\n",
    "                \"ci_covered\": ci_covered,\n",
    "                \"ci_lower_scaled\": ci_lower_scaled.item(),\n",
    "                \"ci_upper_scaled\": ci_upper_scaled.item(),\n",
    "                \"ci_width_scaled\": ci_upper_scaled.item() - ci_lower_scaled.item(),\n",
    "                \"ci_covered_scaled\": ci_lower_scaled.item() <= y_obs[i] <= ci_upper_scaled.item(),\n",
    "                \"pi_lower\": pi_lower.item(),\n",
    "                \"pi_upper\": pi_upper.item(),\n",
    "                \"pi_width\": pi_upper.item() - pi_lower.item(),\n",
    "                \"pi_covered\": pi_covered,\n",
    "                \"pi_lower_scaled\": pi_lower_scaled.item(),\n",
    "                \"pi_upper_scaled\": pi_upper_scaled.item(),\n",
    "                \"pi_width_scaled\": pi_upper_scaled.item() - pi_lower_scaled.item(),\n",
    "                \"pi_covered_scaled\": pi_lower_scaled.item() <= y_obs[i] <= pi_upper_scaled.item(),\n",
    "                \"ri_lower\": ri_lower.item(),\n",
    "                \"ri_upper\": ri_upper.item(),\n",
    "                \"ri_width\": ri_upper.item() - ri_lower.item(),\n",
    "                \"ri_lower_scaled\": ri_lower_scaled.item(),\n",
    "                \"ri_upper_scaled\": ri_upper_scaled.item(),\n",
    "                \"ri_width_scaled\": ri_upper_scaled.item() - ri_lower_scaled.item(),\n",
    "                \"cfml_lower\": cfml_lower.item(),\n",
    "                \"cfml_upper\": cfml_upper.item(),\n",
    "                \"cfml_width\": cfml_upper.item() - cfml_lower.item(),\n",
    "                \"cfml_covered\": cfml_covered,\n",
    "                \"rn_norm\": (rn_norm.item() if type(rn_norm) == np.ndarray else rn_norm),\n",
    "                \"sigma_hat\": sigma_hat.item(),\n",
    "                \"tau_hat\": tau_hat.item(),\n",
    "                \"pi_sigma\": pi_sigma.item()\n",
    "            }\n",
    "\n",
    "    # parallelize training process. \n",
    "    with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "        result = tqdm(executor.map(training_rep, range(rep)), \n",
    "                      desc=f\"Training BRATD (lr={lr}, dr={dr}, sr={sr}, max_depth={max_depth})\", total=rep)\n",
    "        rows = list(result)\n",
    "        executor.shutdown()\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ri coverage rate\n",
    "    for k in range(rep):\n",
    "        for i in range(num_test_pts):\n",
    "            current_row = df[(df[\"model_idx\"] == k) & (df[\"pt_idx\"] == i)]\n",
    "            if current_row.empty:\n",
    "                continue\n",
    "\n",
    "            ri_lower = current_row[\"ri_lower\"].values[0]\n",
    "            ri_upper = current_row[\"ri_upper\"].values[0]\n",
    "            ri_lower_scaled = current_row[\"ri_lower_scaled\"].values[0]\n",
    "            ri_upper_scaled = current_row[\"ri_upper_scaled\"].values[0]\n",
    "\n",
    "            other_models = df[(df[\"model_idx\"] != k) & (df[\"pt_idx\"] == i)]\n",
    "            ri_cov_count = 0\n",
    "            ri_scaled_cov_count = 0\n",
    "\n",
    "            for _, row in other_models.iterrows():\n",
    "                y_pred_other = row[\"y_pred\"]\n",
    "                if ri_lower <= y_pred_other <= ri_upper:\n",
    "                    ri_cov_count += 1\n",
    "                if ri_lower_scaled <= y_pred_other <= ri_upper_scaled:\n",
    "                    ri_scaled_cov_count += 1\n",
    "\n",
    "            df.loc[(df[\"model_idx\"] == k) & (df[\"pt_idx\"] == i), \"ri_coverage\"] = ri_cov_count / (rep - 1)\n",
    "            df.loc[(df[\"model_idx\"] == k) & (df[\"pt_idx\"] == i), \"ri_coverage_scaled\"] = ri_scaled_cov_count / (rep - 1)\n",
    "\n",
    "    # mean coverage rate\n",
    "    mean_coverage = df.groupby(\"pt_idx\").agg(\n",
    "        ci_coverage_mean=(\"ci_covered\", \"mean\"),\n",
    "        ci_scaled_coverage_mean=(\"ci_covered_scaled\", \"mean\"),\n",
    "        pi_coverage_mean=(\"pi_covered\", \"mean\"),\n",
    "        pi_scaled_coverage_mean=(\"pi_covered_scaled\", \"mean\"),\n",
    "        ri_coverage_mean=(\"ri_coverage\", \"mean\"),\n",
    "        ri_scaled_coverage_mean=(\"ri_coverage_scaled\", \"mean\"),\n",
    "        conf_coverage_mean=(\"cfml_covered\", \"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    # merge the mean coverage rate with the original dataframe\n",
    "    df = df.merge(mean_coverage, on=\"pt_idx\", how=\"left\")\n",
    "\n",
    "    # Save results\n",
    "    if not os.path.exists('./coverage/data/'):\n",
    "        os.makedirs('./coverage/data/')\n",
    "    output_filename = f\"npts_{num_test_pts}_rep_{rep}_epo_{epoch}_lr_{lr}_sr_{sr}_dr_{dr}_md_{max_depth}_nys_{Nystrom_subsample}_in_bag_{in_bag}\"\n",
    "    output_path = os.path.join(f\"./coverage/data/\", f\"{output_filename}.parquet\")\n",
    "    df.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below plots the simulated resulst. In particular, it produces the rainclouds plot of interval coverage rate distribution, interval width distribution. We defautly used the calibrated intervals here.The first two rows are computed by averaging over points for an individual model, whereas the last two rows are computed by averaging over models for a individual test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#â€“â€“ Palette & interval-to-label maps â€“â€“\n",
    "palette = {\n",
    "    \"Built-In Confidence Interval\": \"#00BEFF\",\n",
    "    \"Built-In Prediction Interval\": \"#F8766D\",\n",
    "    \"Built-In Reproduction Interval\": \"#7CAE00\",\n",
    "    \"Conformal Prediction Interval\": \"#C77CFF\"\n",
    "}\n",
    "\n",
    "coverage_columns = {\n",
    "    \"ci_covered\": \"Built-In Confidence Interval\",\n",
    "    \"pi_covered\": \"Built-In Prediction Interval\",\n",
    "    \"ri_coverage\": \"Built-In Reproduction Interval\",\n",
    "    \"cfml_covered\": \"Conformal Prediction Interval\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# scaled coverage columns\n",
    "coverage_columns = {\n",
    "    \"ci_covered_scaled\": \"Built-In Confidence Interval\",\n",
    "    \"pi_covered_scaled\": \"Built-In Prediction Interval\",\n",
    "    \"ri_coverage_scaled\": \"Built-In Reproduction Interval\",\n",
    "    \"cfml_covered\": \"Conformal Prediction Interval\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "width_columns = {\n",
    "    \"ci_width\":   \"Built-In Confidence Interval\",\n",
    "    \"pi_width\":   \"Built-In Prediction Interval\",\n",
    "    \"ri_width\":   \"Built-In Reproduction Interval\",\n",
    "    \"cfml_width\": \"Conformal Prediction Interval\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# scaled width columns\n",
    "width_columns = {\n",
    "    \"ci_width_scaled\":   \"Built-In Confidence Interval\",\n",
    "    \"pi_width_scaled\":   \"Built-In Prediction Interval\",\n",
    "    \"ri_width_scaled\":   \"Built-In Reproduction Interval\",\n",
    "    \"cfml_width\": \"Conformal Prediction Interval\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "intervals =[\n",
    "    \"Built-In Confidence Interval\",\n",
    "    \"Built-In Reproduction Interval\",\n",
    "    \"Built-In Prediction Interval\",\n",
    "    \"Conformal Prediction Interval\"\n",
    "]\n",
    "\n",
    "#â€“â€“ Directories â€“â€“\n",
    "input_dir  = \"./coverage/data/\"\n",
    "output_dir = \"./coverage/plots/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#â€“â€“ Loop over every .parquet in the input directory â€“â€“\n",
    "for pq_path in glob.glob(os.path.join(input_dir, \"*.parquet\")):\n",
    "    # 1) Load\n",
    "    df = pd.read_parquet(pq_path)\n",
    "\n",
    "    # 2) Compute averages\n",
    "    # marginal coverage rates (model avg cov / reps)\n",
    "    m_cov_means = (\n",
    "        df\n",
    "        .groupby(\"model_idx\")[ list(coverage_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    m_wid_means = (\n",
    "        df\n",
    "        .groupby(\"model_idx\")[ list(width_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    # conditional coverage rates (per point coverage / test size)\n",
    "    c_cov_means = (\n",
    "        df\n",
    "        .groupby(\"pt_idx\")[ list(coverage_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    c_wid_means = (\n",
    "        df\n",
    "        .groupby(\"pt_idx\")[ list(width_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \"\"\" scaled coverage rates\n",
    "    # marginal coverage rates (model avg cov / reps)\n",
    "    m_cov_means = (\n",
    "        df\n",
    "        .groupby(\"model_idx\")[ list(scaled_coverage_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    m_wid_means = (\n",
    "        df\n",
    "        .groupby(\"model_idx\")[ list(scaled_width_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    # conditional coverage rates (per point coverage / test size)\n",
    "    c_cov_means = (\n",
    "        df\n",
    "        .groupby(\"pt_idx\")[ list(scaled_coverage_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    c_wid_means = (\n",
    "        df\n",
    "        .groupby(\"pt_idx\")[ list(scaled_width_columns.keys()) ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # 3) Melt into long-form coverage & width\n",
    "    m_cov = pd.melt(\n",
    "        m_cov_means,\n",
    "        id_vars=\"model_idx\",\n",
    "        var_name=\"Interval Type\",\n",
    "        value_name=\"Coverage\"\n",
    "    )\n",
    "    m_cov[\"Interval Type\"] = m_cov[\"Interval Type\"].map(coverage_columns)\n",
    "\n",
    "    m_wid = pd.melt(\n",
    "        m_wid_means,\n",
    "        id_vars=\"model_idx\",\n",
    "        var_name=\"Interval Type\",\n",
    "        value_name=\"Width\"\n",
    "    )\n",
    "    m_wid[\"Interval Type\"] = m_wid[\"Interval Type\"].map(width_columns)\n",
    "\n",
    "    c_cov = pd.melt(\n",
    "        c_cov_means,\n",
    "        id_vars=\"pt_idx\",\n",
    "        var_name=\"Interval Type\",\n",
    "        value_name=\"Coverage\"\n",
    "    )\n",
    "    c_cov[\"Interval Type\"] = c_cov[\"Interval Type\"].map(coverage_columns)\n",
    "\n",
    "    c_wid = pd.melt(\n",
    "        c_wid_means,\n",
    "        id_vars=\"pt_idx\",\n",
    "        var_name=\"Interval Type\",\n",
    "        value_name=\"Width\"\n",
    "    )\n",
    "    c_wid[\"Interval Type\"] = c_wid[\"Interval Type\"].map(width_columns)\n",
    "\n",
    "    # 4) Build 4Ã—4 raincloud grid\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 4), sharey=False, sharex=False)\n",
    "\n",
    "    # Each row: (long-form DataFrame, which column to plot on y)\n",
    "    plot_configs = [\n",
    "        (m_cov, \"Coverage\"),    # row 0: marginal coverage (per-model averages, average over points)\n",
    "        (m_wid, \"Width\"),            # row 1: marginal width\n",
    "        (c_cov, \"Coverage\"),    # row 2: conditional coverage (per-point averages, average over models )\n",
    "        (c_wid, \"Width\"),            # row 3: conditional width\n",
    "    ]\n",
    "\n",
    "    ynames = ['Models', 'Models', 'Points', 'Points']\n",
    "    for row_idx, (data_long, y_col) in enumerate(plot_configs):\n",
    "        for col_idx, label in enumerate(intervals):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            sub_df = data_long[data_long[\"Interval Type\"] == label]\n",
    "\n",
    "            # (Optional) annotate outlier ratio only for coverage rows\n",
    "            # if y_col == \"Coverage Rate\":\n",
    "            #     total = len(sub_df)\n",
    "            #     outliers = (sub_df[\"Coverage Rate\"] < 0.9).sum()\n",
    "            #     ratio = outliers / total if total else 0\n",
    "            #     ax.text(\n",
    "            #         0.05, 0.95,\n",
    "            #         f\"Outlier Ratio: {ratio:.2%}\",\n",
    "            #         transform=ax.transAxes,\n",
    "            #         fontsize=7, va=\"top\", ha=\"left\",\n",
    "            #         bbox=dict(boxstyle=\"round,pad=0.3\",\n",
    "            #                   edgecolor=\"grey\", facecolor=\"white\", alpha=0.2)\n",
    "            #     )\n",
    "\n",
    "            # draw the raincloud\n",
    "            pt.RainCloud(\n",
    "                x=\"Interval Type\", y=y_col, data=sub_df,\n",
    "                palette=palette, bw=0.2, width_viol=1.2, width_box=0.2,\n",
    "                ax=ax, orient=\"h\", point_size=1.5, rain_alpha=0.7, jitter=1,\n",
    "                box_linewidth=1, point_linewidth=0.5,\n",
    "                box_whis=(9, 91),\n",
    "                box_whiskerprops=dict(linewidth=0.5),\n",
    "                box_capprops=dict(linewidth=0.5),\n",
    "                box_flierprops=dict(marker=\"o\", markersize=1.0,\n",
    "                                    markeredgewidth=0.5, alpha=0.5)\n",
    "            )\n",
    "\n",
    "            # only the top row shows titles\n",
    "            if row_idx == 0:\n",
    "                ax.set_title(label, fontsize=8)\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel(y_col, fontsize=6)\n",
    "            ax.set_ylabel(ynames[row_idx], fontsize=6)\n",
    "            ax.xaxis.set_major_formatter(FormatStrFormatter('%.3g'))\n",
    "            # # only the bottom row shows x-labels\n",
    "            # if row_idx == 3:\n",
    "            \n",
    "            # else:\n",
    "            #     ax.set_xlabel(\"\")\n",
    "            ax.tick_params(axis=\"x\", labelsize=5, length=2, width=0.5)\n",
    "\n",
    "    plt.tight_layout() #(pad=2.0)\n",
    "    base = os.path.splitext(os.path.basename(pq_path))[0]\n",
    "    out_path = os.path.join(output_dir, f\"{base}.png\")\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    print('saved')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Saved raincloud plots to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
